{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multipledispatch import dispatch \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def corr_heatmap(df):\n",
    "    corr = df.corr()\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    sns.set(font_scale=1.15)\n",
    "    with sns.axes_style(style=\"ticks\"):\n",
    "        f, ax = plt.subplots(figsize=(7, 5))\n",
    "    ax = sns.heatmap(corr, mask=mask, annot=True, linewidths=1, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_accuracy(actual, pred):\n",
    "    assert len(actual) == len(pred)\n",
    "    correct = 0\n",
    "    for i, j in zip(actual, pred):\n",
    "        if i == j:\n",
    "            correct += 1\n",
    "    return correct / len(actual)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10, min_rating=4.0):\n",
    "    top_n = defaultdict(list)\n",
    "\n",
    "    for user_id, business_id, actual_rating, estimated_rating, _ in predictions:\n",
    "        if (estimated_rating >= min_rating):\n",
    "            top_n[user_id].append((business_id, estimated_rating))\n",
    "\n",
    "    for user_id, ratings in top_n.items():\n",
    "        ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[user_id] = ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genres(data):\n",
    "    genres = defaultdict(list)\n",
    "    genreIDs = {}\n",
    "    maxGenreID = 0\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        row = data.iloc[i]\n",
    "        businessID = row[\"business_id\"]\n",
    "        genreList = row[\"business_categories\"].split(',')\n",
    "        genreIDList = []\n",
    "        for genre in genreList:\n",
    "            if genre in genreIDs:\n",
    "                genreID = genreIDs[genre]\n",
    "            else:\n",
    "                genreID = maxGenreID\n",
    "                genreIDs[genre] = genreID\n",
    "                maxGenreID += 1\n",
    "            genreIDList.append(genreID)\n",
    "        genres[businessID] = genreIDList\n",
    "\n",
    "        \n",
    "    for (businessID, genreIDList) in genres.items():\n",
    "        bitfield = [0] * maxGenreID\n",
    "        for genreID in genreIDList:\n",
    "            bitfield[genreID] = 1\n",
    "        genres[businessID] = bitfield            \n",
    "\n",
    "    return genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dispatch(np.ndarray,pd.core.series.Series)\n",
    "def evaluate(y_pred, y_test):\n",
    "#     y_pred = model.predict(X_test)\n",
    "\n",
    "    print('\\nLinear Regression Performance Metrics')\n",
    "    print('R^2=', explained_variance_score(y_test,y_pred))\n",
    "    print('MAE:', mean_absolute_error(y_test,y_pred))\n",
    "    print('MSE:', mean_squared_error(y_test,y_pred))\n",
    "    print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dispatch(object,list)\n",
    "def evaluate(model, testset):\n",
    "    print(\"\\nComputing recommendations...\")\n",
    "    predictions = model.test(testset)\n",
    "\n",
    "    print(\"\\nEvaluating accuracy of model...\")\n",
    "    print(\"RMSE: \", accuracy.mae(predictions, verbose=False))\n",
    "    print(\"MAE: \", accuracy.rmse(predictions, verbose=False))\n",
    "#     print(\"ACC: \", macro_accuracy([i[2] for i in testset], [predictions[i].r_ui for i in range(len(predictions))]))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, tokenizer):\n",
    "   tokens = list(tokenizer.tokenize(s))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(X, tokenizer):\n",
    "  num_examples = len(X)\n",
    "  \n",
    "  sentence1 = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(X)])\n",
    "\n",
    "#   sentence2 = tf.ragged.constant([\n",
    "#       encode_sentence(s, tokenizer)\n",
    "#        for s in np.array(glue_dict[\"sentence2\"])])\n",
    "\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
    "  input_word_ids = tf.concat([cls, sentence1], axis=-1)\n",
    "\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_s1 = tf.zeros_like(sentence1)\n",
    "#   type_s2 = tf.ones_like(sentence2)\n",
    "  input_type_ids = tf.concat([type_cls, type_s1], axis=-1).to_tensor()\n",
    "\n",
    "  inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
